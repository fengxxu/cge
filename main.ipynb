{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import z3\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(columns=['bn','mo', 'pro', 'ar','at','mr','mt','mar','mat','nr','nt','vr','vt','pr','pt','cr','ct','dr','dt'])\n",
    "ins = 'instances.csv'\n",
    "result = 'results.csv'\n",
    "benchmarks=['acasxu','collins_rul_cnn','mnist_fc','rl_benchmarks','tllverifybench','traffic_signs_recognition']\n",
    "\n",
    "benchmarks_paths={'acasxu': './benchmarks/acasxu',\n",
    "                  'collins_rul_cnn':'./benchmarks/collins_rul_cnn',\n",
    "                  'mnist_fc':'./benchmarks/mnist_fc',\n",
    "                  'rl_benchmarks':'./benchmarks/rl_benchmarks', \n",
    "                  'tllverifybench':'./benchmarks/tllverifybench',\n",
    "                  'traffic_signs_recognition':'./benchmarks/traffic_signs_recognition'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_benchmark_csv_add_to_final(benchmark_name, benchmark_path, prefix):\n",
    "\n",
    "    with open(benchmark_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            model =os.path.join(prefix,row[0])\n",
    "            prop = os.path.join(prefix,row[1])\n",
    "            time_out = row[2]\n",
    "            final.loc[len(final)] = { 'bn':benchmark_name, 'mo':model,'pro': prop}\n",
    "\n",
    "def all_benchmarks():\n",
    "    for i in benchmarks_paths:\n",
    "        path = os.path.join(benchmarks_paths[i], ins)\n",
    "        prefix = './benchmarks/'+i\n",
    "        # print(i,path,prefix)\n",
    "        read_benchmark_csv_add_to_final(i,path,prefix)\n",
    "    \n",
    "\n",
    "all_benchmarks()\n",
    "print(final.to_string())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_write_verification_result(result_csv_path,key_1,key_2):\n",
    "    '''Read each verifier result csv file and add to the dataframe final '''\n",
    "    with open(result_csv_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            benchmark_name = row[0]\n",
    "            if benchmark_name in benchmarks:\n",
    "                # print(benchmark_name)\n",
    "                condition_model = row[1]\n",
    "                # print(condition_model)\n",
    "                condition_prop = row[2]\n",
    "                # print(condition_prop)\n",
    "                if row[4]=='sat' or row[4]=='unsat':\n",
    "                    ver_res = row[4]\n",
    "                else:\n",
    "                    ver_res = 'unknown'\n",
    "                ver_time = row[5]\n",
    "                final.loc[(final['bn']==benchmark_name )& (final['mo']==condition_model) & (final['pro']==condition_prop), key_1] = ver_res\n",
    "                final.loc[(final['bn']==benchmark_name )& (final['mo']==condition_model)  & (final['pro']==condition_prop), key_2] = ver_time\n",
    "\n",
    "ce_results = pd.DataFrame(columns=['bn','mo', 'pro', 'ca','cm','cma','cn','cv','cp','cc','cd'])\n",
    "results_path = {'abcrown':'./verification_results/abcrown',\n",
    "                'cgdtest':'./verification_results/cgdtest',\n",
    "                'debona':'./verification_results/marabou',\n",
    "                'marabou':'./verification_results/marabou',\n",
    "                'mn_bab':'./verification_results/mn_bab',\n",
    "                'nnenum':'./verification_results/nnenum',\n",
    "                'peregrinn':'./verification_results/peregrinn',\n",
    "                'verinet':'./verification_results/verinet'\n",
    "                }\n",
    "\n",
    "\n",
    "def add_results():\n",
    "    path_1 = './verification_results/abcrown/results.csv'\n",
    "    path_2 = './verification_results/cgdtest/results.csv'\n",
    "    path_3 = './verification_results/marabou/results.csv'\n",
    "    path_4 = './verification_results/debona/results.csv'\n",
    "    path_5 = './verification_results/mn_bab/results.csv'\n",
    "    path_6 = './verification_results/peregrinn/results.csv'\n",
    "    path_7 = './verification_results/verinet/results.csv'\n",
    "    path_8 = './verification_results/nnenum/results.csv'\n",
    "    read_and_write_verification_result(path_1,'ar','at')\n",
    "    read_and_write_verification_result(path_2,'cr','ct')\n",
    "    read_and_write_verification_result(path_3,'mar','mat')\n",
    "    read_and_write_verification_result(path_4,'dr','dt')\n",
    "    read_and_write_verification_result(path_5,'mr','mt')\n",
    "    read_and_write_verification_result(path_6,'pr','pt')\n",
    "    read_and_write_verification_result(path_7,'vr','vt')\n",
    "    read_and_write_verification_result(path_8,'nr','nt')\n",
    "    print(final.to_string())\n",
    "add_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell only when check the stablization\n",
    "\n",
    "# precentage : 1, 0.8, 0.6, 0.4,\n",
    "# precentage = 1\n",
    "# random_n = int(711 * precentage)\n",
    "# final_2 = final.sample(n=random_n)\n",
    "# final = final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ce_file(ce_path):\n",
    "\n",
    "    if ce_path.endswith('.gz'):\n",
    "        with gzip.open(ce_path, 'rb') as f:\n",
    "            content = f.read().decode('utf-8')\n",
    "    else:\n",
    "        with open(ce_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "    content = content.replace('\\n', ' ').strip()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# check counterexamples\n",
    "\n",
    "\n",
    "\n",
    "map_2 = {'abcrown':'ca','cgdtest':'cc','debona':'cd','marabou':'cma','mn_bab':'cm','nnenum':'cn','peregrinn':'cp','verinet':'cv'}\n",
    "\n",
    "def read_ce(filename: str):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    input = list()\n",
    "    for line in content:\n",
    "        if \"X\" in line: \n",
    "            line = line.replace(\"(\", \"\").replace(\")\",\"\").replace(\",\", \"\")\n",
    "            value = line.split()[-1]\n",
    "            input.append(float(value))\n",
    "    f.close()\n",
    "    return input\n",
    "\n",
    "\n",
    "def inference_network(model_path:str, ce_input: list):\n",
    "    input_data = np.array(ce_input,dtype=np.float32)\n",
    "    sess = onnxruntime.InferenceSession(model_path)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    input_shape = sess.get_inputs()[0].shape\n",
    "    for i in range(len(input_shape)):\n",
    "            if not isinstance(input_shape[i],int):\n",
    "                input_shape[i]=1\n",
    "    input_data = input_data.reshape(input_shape)\n",
    "    if input_data.shape != tuple(input_shape):\n",
    "        raise ValueError(f\"Input data shape does not match the model. Expected: {input_shape}, Got: {input_data.shape}\")\n",
    "    outputs = sess.run(None, {input_name: input_data})\n",
    "    output = outputs[0]\n",
    "    return output\n",
    "\n",
    "\n",
    "def read_properties(prop_path:str):\n",
    "    with open(prop_path, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    y_properties = \" \"\n",
    "    add_output_flag = False\n",
    "    for line in content:\n",
    "        if \"Y\" in line and \"declare\" in line: \n",
    "            y_properties += line\n",
    "        elif \"Output constraints\" in line or \"unsafe\" in line or \"coc\" in line or \"Unsafe\" in line:\n",
    "            add_output_flag = True\n",
    "        elif add_output_flag == True:\n",
    "            y_properties += line\n",
    "    f.close()\n",
    "    return y_properties\n",
    "\n",
    "def reason(y_properties, output):\n",
    "    # collectY_network_list = [\"(assert (= Y_{} {}))\".format(i, output[i]) for i in range(len(output))]\n",
    "    collectY_network_list = list()\n",
    "    for j in range(len(output)):\n",
    "        single_value = output[j] # iteration over array\n",
    "        if \"e\" in str(single_value):\n",
    "            r_value = str(single_value).split(\"e\")[0]\n",
    "            res = int(str(single_value).split(\"e\")[1])\n",
    "            collectY_network_list.append(\"(assert (= Y_{} (* {} (^ 10 {}))))\".format(j, float(r_value), res))\n",
    "        else:\n",
    "            collectY_network_list.append(\"(assert (= Y_{} {}))\".format(j, single_value))\n",
    "    collectY_network_output_constraint = \"\".join(collectY_network_list)\n",
    "    combine = y_properties + collectY_network_output_constraint\n",
    "    combine = combine.replace(\"\\n\", \"\").encode(encoding='utf-8')\n",
    "    solver = z3.Solver()\n",
    "    solver.reset()\n",
    "    try:   \n",
    "        SMT_properties = z3.parse_smt2_string(combine)\n",
    "        solver.add(SMT_properties)\n",
    "    except z3.Z3Exception:\n",
    "        print(f\"Error {combine}\")\n",
    "    return solver.check()\n",
    "\n",
    "base = '/home/feng/vnncomp2022_benchmarks'\n",
    "\n",
    "\n",
    "def add_to_ce_results(benchmark_name,model,prop):\n",
    "    ce_results.loc[len(ce_results)] = {'bn':benchmark_name,'mo':model,'pro':prop}\n",
    "\n",
    "\n",
    "def find_counterexample_paths():\n",
    "    for i, r in final.iterrows():\n",
    "        benchmark_name = r['bn']\n",
    "        model = r['mo']\n",
    "        prop = r['pro']\n",
    "        model_name = model.split('/')[-1][:-5]\n",
    "        prop_name = prop.split('/')[-1][:-7]\n",
    "        ce_name = model_name+'_'+prop_name+'.counterexample'\n",
    "        # ar = \n",
    "        if r['ar'] == 'sat':\n",
    "            add_to_ce_results(benchmark_name,model,prop)\n",
    "        # if r['bn']=='traffic_signs_recognition':\n",
    "        #     continue\n",
    "        if r['ar'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'abcrown')\n",
    "        if r['mr'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'mn_bab')\n",
    "        if r['mar'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'marabou')\n",
    "        if r['nr'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'nnenum')\n",
    "        if r['vr'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'verinet')\n",
    "        if r['pr'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'peregrinn')\n",
    "        if r['cr'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'cgdtest')\n",
    "        if r['dr'] == 'sat':\n",
    "            check_counterexample(benchmark_name,ce_name,model,prop,'debona')\n",
    "        \n",
    "\n",
    "def additional_check():\n",
    "    temp = 0 \n",
    "    tem = 0     \n",
    "    for i, r in ce_results.iterrows():\n",
    "        if r['bn']=='traffic_signs_recognition':\n",
    "            temp +=1\n",
    "            if temp in [14,21,27]:\n",
    "                ce_results.at[i,'ca']='error'\n",
    "            else:\n",
    "                ce_results.at[i,'ca']='correct'\n",
    "            tem +=1\n",
    "            if tem ==27:\n",
    "                ce_results.at[i,'cma']='error'\n",
    "            if tem in [1,2,3,4,5,10,11,12,13,18,24,25,26,28,33,40,41,42]:\n",
    "                ce_results.at[i,'cma']='correct'\n",
    "\n",
    "\n",
    "\n",
    "def check_counterexample(benchmark_name, ce_name, model, prop, c):\n",
    "    ce_path = os.path.join(results_path[c],benchmark_name,ce_name)\n",
    "    if os.path.exists(ce_path):\n",
    "        model_path = base+model[1:]\n",
    "        print(model_path)\n",
    "        prop_path = base+prop[1:]\n",
    "        input = read_ce(ce_path)\n",
    "        if len(input) <2:\n",
    "            ce_results.loc[(ce_results['bn']==benchmark_name)& (ce_results['mo']==model)& (ce_results['pro']==prop), map_2[c]] = 'error'\n",
    "        else:\n",
    "            network_output = inference_network(model_path,input)\n",
    "            y_properties = read_properties(prop_path)\n",
    "            try:\n",
    "                check_result = reason(y_properties,network_output)\n",
    "            \n",
    "                if check_result == z3.unsat:\n",
    "                    ce_results.loc[(ce_results['bn']==benchmark_name)& (ce_results['mo']==model)& (ce_results['pro']==prop), map_2[c]] = \"error\"\n",
    "                    # ce_results.loc[len(ce_results)] = {'bn':benchmark_name,'mo':model,'pro':prop,map_2[c]:'error'}\n",
    "                elif check_result == z3.sat:\n",
    "                    ce_results.loc[(ce_results['bn']==benchmark_name)& (ce_results['mo']==model)& (ce_results['pro']==prop), map_2[c]] = \"correct\"\n",
    "                    \n",
    "                    # ce_results.loc[len(ce_results)] = {'bn':benchmark_name,'mo':model,'pro':prop,map_2[c]:'correct'}\n",
    "                else:\n",
    "                    ce_results.loc[(ce_results['bn']==benchmark_name)& (ce_results['mo']==model)& (ce_results['pro']==prop), map_2[c]] = \"???\"\n",
    "\n",
    "                    # ce_results.loc[len(ce_results)] = {'bn':benchmark_name,'mo':model,'pro':prop,map_2[c]:'??'}\n",
    "            except:\n",
    "                ce_results.loc[(ce_results['bn']==benchmark_name)& (ce_results['mo']==model)& (ce_results['pro']==prop), map_2[c]] = \"?\"\n",
    "\n",
    "                # ce_results.loc[len(ce_results)] = {'bn':benchmark_name,'mo':model,'pro':prop,map_2[c]:'?'}\n",
    "    additional_check()        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def report_ce(data: pd.DataFrame, toolsname:str):\n",
    "    # conservative_report_pass(data, toolsname)\n",
    "    # agressive_report_pass(data, toolsname)\n",
    "    # agressive_report_nonrobust(data, toolsname)\n",
    "    # conservative_report_nonrobust(data, toolsname)\n",
    "    data = data[data[\"result\"] ==\"sat\"]\n",
    "    fp_input = 0\n",
    "    for index, row in data.iterrows():\n",
    "        ce_path = os.path.splitext(row[\"network_path\"])[0] \\\n",
    "            +\"_\" + os.path.splitext(os.path.basename(row[\"property_path\"]))[0] \\\n",
    "            + \".counterexample\"\n",
    "        ce_path = ce_path.replace(\"benchmarks\", \"survey_data/{}\".format(toolsname), 1).replace(\"onnx/\", \"\")\n",
    "        libproperty = read_properties(row[\"property_path\"])\n",
    "        input = read_ce(ce_path)\n",
    "        if len(input) ==0:\n",
    "            continue\n",
    "        output = inference_network(row[\"network_path\"], input)\n",
    "        check_N = reason(libproperty, output[0])\n",
    "        if check_N == z3.unsat:\n",
    "            fp_input += 1\n",
    "            with open(f\"{toolsname}_Verification_input_fp.txt\", \"a\") as fileA:\n",
    "                # fileA.write(\"\\n\" + row.to_json())\n",
    "                fileA.write(f\"property_path:\\t {ce_path} \\n network_outputs:\\t {output} \\n\")\n",
    "        if check_N == z3.sat:\n",
    "            lines = [f\"{toolsname}\", f\"{row['network_path']}\", f\"{row['property_path']}\", f\"{ce_path}\"]\n",
    "            with open(f\"{data['application'].iloc[1]}_real_ce.csv\", mode= \"a\") as fileB:\n",
    "                writer = csv.writer(fileB)\n",
    "                writer.writerow(lines)\n",
    "    print(f\"input fp: {fp_input} using {toolsname} for {data['application'].iloc[1]} \\n\")\n",
    "    return\n",
    "find_counterexample_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_results.fillna('unknown', inplace=True)\n",
    "print(ce_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violate_ground_truth = pd.DataFrame(columns=['bn','mo', 'pro', 'gt'])\n",
    "for index, row in ce_results.iterrows():\n",
    "    if row['ca']=='correct' or row['cm']=='correct' or row['cma']=='correct' or row['cn']=='correct' or row['cv']=='correct' or row['cp']=='correct' or row['cc']=='correct' or row['cd']=='correct':\n",
    "        violate_ground_truth.loc[len(violate_ground_truth)] = {'bn':row['bn'],'mo':row['mo'],'pro':row['pro'],'gt':'sat'}\n",
    "print(violate_ground_truth.to_string())\n",
    "\n",
    "desirable_ground_truth = pd.DataFrame(columns=['bn','mo', 'pro', 'gt'])\n",
    "for index, row in final.iterrows():\n",
    "    if row['ar']=='sat' or row['mr']=='sat' or row['mar']=='sat' or row['nr']=='sat' or row['vr']=='sat' or row['pr']=='sat' or row['cr']=='sat' or row['dr']=='sat':\n",
    "        continue\n",
    "    if row['ar']=='unsat' or row['mr']=='unsat' or row['mar']=='unsat' or row['nr']=='unsat' or row['vr']=='unsat' or row['pr']=='unsat' or row['cr']=='unsat' or row['dr']=='unsat':\n",
    "        desirable_ground_truth.loc[len(desirable_ground_truth)] = {'bn':row['bn'],'mo':row['mo'],'pro':row['pro'],'gt':'unsat'}\n",
    "\n",
    "print(desirable_ground_truth.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soundness and completeness ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_1 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_2 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_3 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_4 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_time_1 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_time_2 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_time_3 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "count_time_4 ={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "for index, row in violate_ground_truth.iterrows():\n",
    "    model = row['mo']\n",
    "    prop = row['pro']\n",
    "    # find the row in final based on model and prop\n",
    "    value_final = final.loc[(final['mo']==model)& (final['pro']==prop)]\n",
    "    if value_final['ar'].iloc[0]=='unsat' or value_final['ar'].iloc[0]=='unknown':\n",
    "        count_2['abcrown']+=1\n",
    "        count_time_2['abcrown']+=float(value_final['at'].iloc[0])\n",
    "    if value_final['mr'].iloc[0]=='unsat' or value_final['mr'].iloc[0]=='unknown':\n",
    "        count_2['mnbab']+=1\n",
    "        count_time_2['mnbab']+=float(value_final['mt'].iloc[0])\n",
    "    if value_final['mar'].iloc[0]=='unsat' or value_final['mar'].iloc[0]=='unknown':\n",
    "        count_2['marabou']+=1\n",
    "        count_time_2['marabou']+=float(value_final['mat'].iloc[0])\n",
    "    if value_final['vr'].iloc[0]=='unsat' or value_final['vr'].iloc[0]=='unknown':\n",
    "        count_2['verinet']+=1\n",
    "        count_time_2['verinet']+=float(value_final['vt'].iloc[0])\n",
    "    if value_final['nr'].iloc[0]=='unsat' or value_final['nr'].iloc[0]=='unknown':\n",
    "        count_2['nnenum']+=1\n",
    "        count_time_2['nnenum']+=float(value_final['nt'].iloc[0])\n",
    "    if value_final['pr'].iloc[0]=='unsat' or value_final['pr'].iloc[0]=='unknown':\n",
    "        count_2['peregrinn']+=1\n",
    "        count_time_2['peregrinn']+=float(value_final['pt'].iloc[0])\n",
    "    if value_final['cr'].iloc[0]=='unsat' or value_final['cr'].iloc[0]=='unknown':\n",
    "        count_2['cgdtest']+=1\n",
    "        count_time_2['cgdtest']+=float(value_final['ct'].iloc[0])\n",
    "    if value_final['dr'].iloc[0]=='unsat' or value_final['dr'].iloc[0]=='unknown':\n",
    "        count_2['debona']+=1\n",
    "        count_time_2['debona']+=float(value_final['dt'].iloc[0])\n",
    "    if value_final['ar'].iloc[0]=='sat':\n",
    "        count_3['abcrown']+=1\n",
    "        count_time_3['abcrown']+=float(value_final['at'].iloc[0])\n",
    "    if value_final['mr'].iloc[0]=='sat':\n",
    "        count_3['mnbab']+=1\n",
    "        count_time_3['mnbab']+=float(value_final['mt'].iloc[0])\n",
    "    if value_final['mar'].iloc[0]=='sat':\n",
    "        count_3['marabou']+=1\n",
    "        count_time_3['marabou']+=float(value_final['mat'].iloc[0])\n",
    "    if value_final['vr'].iloc[0]=='sat':\n",
    "        count_3['verinet']+=1\n",
    "        count_time_3['verinet']+=float(value_final['vt'].iloc[0])\n",
    "    if value_final['nr'].iloc[0]=='sat':\n",
    "        count_3['nnenum']+=1\n",
    "        count_time_3['nnenum']+=float(value_final['nt'].iloc[0])\n",
    "    if value_final['pr'].iloc[0]=='sat':\n",
    "        count_3['peregrinn']+=1\n",
    "        count_time_3['peregrinn']+=float(value_final['pt'].iloc[0])\n",
    "    if value_final['cr'].iloc[0]=='sat':\n",
    "        count_3['cgdtest']+=1\n",
    "        count_time_3['cgdtest']+=float(value_final['ct'].iloc[0])\n",
    "    if value_final['dr'].iloc[0]=='sat':\n",
    "        count_3['debona']+=1\n",
    "        count_time_3['debona']+=float(value_final['dt'].iloc[0])\n",
    "\n",
    "for index, row in desirable_ground_truth.iterrows():\n",
    "    model = row['mo']\n",
    "    prop = row['pro']\n",
    "    value_final = final.loc[(final['mo']==model)& (final['pro']==prop)]\n",
    "    if value_final['ar'].iloc[0]=='unsat' or value_final['ar'].iloc[0]=='unknown':\n",
    "        count_1['abcrown']+=1\n",
    "        count_time_1['abcrown']+=float(value_final['at'].iloc[0])\n",
    "    if value_final['mr'].iloc[0]=='unsat' or value_final['mr'].iloc[0]=='unknown':\n",
    "        count_1['mnbab']+=1\n",
    "        count_time_1['mnbab']+=float(value_final['mt'].iloc[0])\n",
    "    if value_final['mar'].iloc[0]=='unsat' or value_final['mar'].iloc[0]=='unknown':\n",
    "        count_1['marabou']+=1\n",
    "        count_time_1['marabou']+=float(value_final['mat'].iloc[0])\n",
    "    if value_final['vr'].iloc[0]=='unsat' or value_final['vr'].iloc[0]=='unknown':\n",
    "        count_1['verinet']+=1\n",
    "        count_time_1['verinet']+=float(value_final['vt'].iloc[0])\n",
    "    if value_final['nr'].iloc[0]=='unsat' or value_final['nr'].iloc[0]=='unknown':\n",
    "        count_1['nnenum']+=1\n",
    "        count_time_1['nnenum']+=float(value_final['nt'].iloc[0])\n",
    "    if value_final['pr'].iloc[0]=='unsat' or value_final['pr'].iloc[0]=='unknown':\n",
    "        count_1['peregrinn']+=1\n",
    "        count_time_1['peregrinn']+=float(value_final['pt'].iloc[0])\n",
    "    if value_final['cr'].iloc[0]=='unsat' or value_final['cr'].iloc[0]=='unknown':\n",
    "        count_1['cgdtest']+=1\n",
    "        count_time_1['cgdtest']+=float(value_final['ct'].iloc[0])\n",
    "    if value_final['dr'].iloc[0]=='unsat' or value_final['dr'].iloc[0]=='unknown':\n",
    "        count_1['debona']+=1\n",
    "        count_time_1['debona']+=float(value_final['dt'].iloc[0])\n",
    "\n",
    "    if value_final['ar'].iloc[0]=='sat' or value_final['ar'].iloc[0]=='unknown':\n",
    "        count_4['abcrown']+=1\n",
    "        count_time_4['abcrown']+=float(value_final['at'].iloc[0])\n",
    "    if value_final['mr'].iloc[0]=='sat' or value_final['mr'].iloc[0]=='unknown':\n",
    "        count_4['mnbab']+=1\n",
    "        count_time_4['mnbab']+=float(value_final['mt'].iloc[0])\n",
    "    if value_final['mar'].iloc[0]=='sat' or value_final['mar'].iloc[0]=='unknown':\n",
    "        count_4['marabou']+=1\n",
    "        count_time_4['marabou']+=float(value_final['mat'].iloc[0])\n",
    "    if value_final['vr'].iloc[0]=='sat' or value_final['vr'].iloc[0]=='unknown':\n",
    "        count_4['verinet']+=1\n",
    "        count_time_4['verinet']+=float(value_final['vt'].iloc[0])\n",
    "    if value_final['nr'].iloc[0]=='sat' or value_final['nr'].iloc[0]=='unknown':\n",
    "        count_4['nnenum']+=1\n",
    "        count_time_4['nnenum']+=float(value_final['nt'].iloc[0])\n",
    "    if value_final['pr'].iloc[0]=='sat' or value_final['pr'].iloc[0]=='unknown':\n",
    "        count_4['peregrinn']+=1\n",
    "        count_time_4['peregrinn']+=float(value_final['pt'].iloc[0])\n",
    "    if value_final['cr'].iloc[0]=='sat' or value_final['cr'].iloc[0]=='unknown':\n",
    "        count_4['cgdtest']+=1\n",
    "        count_time_4['cgdtest']+=float(value_final['ct'].iloc[0])\n",
    "    if value_final['dr'].iloc[0]=='sat' or value_final['dr'].iloc[0]=='unknown':\n",
    "        count_4['debona']+=1\n",
    "        count_time_4['debona']+=float(value_final['dt'].iloc[0])\n",
    "\n",
    "print(count_1,'\\n',count_2,'\\n',count_3,'\\n',count_4,'\\n')\n",
    "print(count_time_1,'\\n',count_time_2,'\\n',count_time_3,'\\n',count_time_4,'\\n')\n",
    "\n",
    "soundness={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "completeness={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "a_mean_sound_time={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "a_mean_complete_time={'abcrown':0,'mnbab':0, 'marabou':0, 'verinet':0,'nnenum':0,'peregrinn':0,'debona':0,'cgdtest':0}\n",
    "for i in count_1:\n",
    "    soundness[i] = count_1[i]/(count_1[i]+count_2[i])\n",
    "    completeness[i] = count_3[i]/(count_3[i]+ count_4[i])\n",
    "    a_mean_sound_time[i] = (count_time_1[i]+count_time_2[i])/(count_1[i]+count_2[i])\n",
    "    a_mean_complete_time[i] = (count_time_3[i]+count_time_4[i])/(count_3[i]+count_4[i])\n",
    "\n",
    "print(\"soundness: \")\n",
    "print(soundness)\n",
    "print(\"completeness: \")\n",
    "print(completeness)\n",
    "\n",
    "print('\\n')\n",
    "print(\"mean sound time: \", a_mean_sound_time)\n",
    "print(\"mean complete time: \", a_mean_complete_time)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute all cell above for the performance of the verifiers. Please note precentage in cell 5 should be 1.\n",
    "\n",
    "\n",
    "### The following cells are used for the stablization evluation, change the precentage value to 0.8, 0.6 and 0.4 in cell 5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer to Dataframe type\n",
    "soundness_df = pd.DataFrame(list(soundness.items()),columns = ['verifier','soundness'])\n",
    "completeness_df = pd.DataFrame(list(completeness.items()),columns = ['verifier','completeness'])\n",
    "\n",
    "\n",
    "# instore to csv file\n",
    "soundness_df.to_csv('./results/soundness_A.csv', mode='a', header=False, index=False)\n",
    "completeness_df.to_csv('./results/completeness_A.csv', mode='a',  header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the stablization, the experiment data in the paper runs above cells 10 times. However, you can run as many time as you like. After running above cells, run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/completeness_A.csv')\n",
    "grouped = df.groupby('verifier')['completeness']\n",
    "means = grouped.mean()\n",
    "print(means)\n",
    "variances = grouped.var()\n",
    "print(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/soundness_A.csv')\n",
    "grouped = df.groupby('verifier')['soundness']\n",
    "means = grouped.mean()\n",
    "print(means)\n",
    "variances = grouped.var()\n",
    "print(variances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
